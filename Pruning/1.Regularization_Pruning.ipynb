{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "1.Regularization_Pruning.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ektagavas/CVSummerSchool2021/blob/main/Pruning/1.Regularization_Pruning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hFW6Wn2QdQvA"
      },
      "source": [
        "# Adding a regularizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_Ni7vmjdQvC"
      },
      "source": [
        "<img src=\"https://github.com/Ektagavas/CVSummerSchool2021/blob/main/Pruning/images/reg1.png?raw=1\">\n",
        "\n",
        "While our loss function allows us to determine how well (or poorly) our set of parameters (i.e., weight matrix, and bias vector) are performing on a given classification task, the loss function itself does not take into account how the weight matrix “looks”. This brings us to the following queations.  How do we go about choosing a set of parameters that will help ensure our model generalizes well? Or at the very least, lessen the affects of overfitting?\n",
        "\n",
        "The answer is <b>regularization.</b>\n",
        "\n",
        "This notebook is divided into two parts. In the first part we will observe the behaviour of the weight space after adding adding a regularization term while in the second part, we will use some property of the learnt weight space to remove redundant connections.\n",
        "\n",
        "Let us begin by our regular import statements."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-08-22T03:31:21.017874Z",
          "iopub.status.busy": "2021-08-22T03:31:21.017393Z",
          "iopub.status.idle": "2021-08-22T03:31:22.100284Z",
          "shell.execute_reply": "2021-08-22T03:31:22.099747Z",
          "shell.execute_reply.started": "2021-08-22T03:31:21.017802Z"
        },
        "tags": [],
        "id": "kJZxX1AcdQvF"
      },
      "source": [
        "import numpy as np\n",
        "import torch \n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torchvision.datasets as dsets\n",
        "import torchvision.transforms as transforms\n",
        "from os.path import join\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pg2RXjPMdQvG"
      },
      "source": [
        "### Hyperparameters to be used"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-08-22T03:49:56.198016Z",
          "iopub.status.busy": "2021-08-22T03:49:56.197802Z",
          "iopub.status.idle": "2021-08-22T03:49:56.201542Z",
          "shell.execute_reply": "2021-08-22T03:49:56.200970Z",
          "shell.execute_reply.started": "2021-08-22T03:49:56.197993Z"
        },
        "tags": [],
        "id": "uuWJZ8rkdQvH"
      },
      "source": [
        "num_epochs = 5\n",
        "batch_size = 100\n",
        "learning_rate = 0.001\n",
        "use_reg = False\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using {device} device\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8FX6PqoMdQvI"
      },
      "source": [
        "### Downloading the MNIST Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-08-22T03:48:34.656357Z",
          "iopub.status.busy": "2021-08-22T03:48:34.656108Z",
          "iopub.status.idle": "2021-08-22T03:48:34.700381Z",
          "shell.execute_reply": "2021-08-22T03:48:34.699846Z",
          "shell.execute_reply.started": "2021-08-22T03:48:34.656329Z"
        },
        "jupyter": {
          "outputs_hidden": false
        },
        "tags": [],
        "id": "ILz3rWIfdQvK"
      },
      "source": [
        "train_dataset = dsets.MNIST(root='.',\n",
        "                            train=True, \n",
        "                            transform=transforms.ToTensor(),\n",
        "                            download=True)\n",
        "\n",
        "test_dataset = dsets.MNIST(root='.',\n",
        "                           train=False, \n",
        "                           transform=transforms.ToTensor(),\n",
        "\t\t\t\t\t\tdownload=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6aEqLpCjdQvL"
      },
      "source": [
        "### Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-08-22T03:48:37.770313Z",
          "iopub.status.busy": "2021-08-22T03:48:37.768276Z",
          "iopub.status.idle": "2021-08-22T03:48:37.774823Z",
          "shell.execute_reply": "2021-08-22T03:48:37.774353Z",
          "shell.execute_reply.started": "2021-08-22T03:48:37.770288Z"
        },
        "tags": [],
        "id": "hVpj3t9odQvM"
      },
      "source": [
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
        "                                           batch_size=batch_size, \n",
        "                                           shuffle=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
        "                                          batch_size=batch_size, \n",
        "                                          shuffle=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rUMM6z4fdQvN"
      },
      "source": [
        "### Defining the network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-08-22T03:48:40.275723Z",
          "iopub.status.busy": "2021-08-22T03:48:40.273760Z",
          "iopub.status.idle": "2021-08-22T03:48:40.283858Z",
          "shell.execute_reply": "2021-08-22T03:48:40.283231Z",
          "shell.execute_reply.started": "2021-08-22T03:48:40.275696Z"
        },
        "tags": [],
        "id": "WLobxYAxdQvO"
      },
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.layer1 = nn.Sequential(\n",
        "            nn.Conv2d(1, 16, kernel_size=5, padding=2),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.ReLU())\n",
        "        self.layer2 = nn.Sequential(\n",
        "            nn.Conv2d(16, 16, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2))\n",
        "        self.layer3 = nn.Sequential(\n",
        "            nn.Conv2d(16, 32, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2))\n",
        "        self.fc1 = nn.Linear(7*7*32, 300)\n",
        "        self.fc2 = nn.Linear(300, 10)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        out = self.layer1(x)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.fc1(out)\n",
        "        out = self.fc2(out)\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DZ3amhm8dQvP"
      },
      "source": [
        "<b>The below function is called to reinitialize the weights of the network and define the required loss criterion and the optimizer.</b> "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-08-22T03:48:42.216635Z",
          "iopub.status.busy": "2021-08-22T03:48:42.214669Z",
          "iopub.status.idle": "2021-08-22T03:48:42.221443Z",
          "shell.execute_reply": "2021-08-22T03:48:42.220857Z",
          "shell.execute_reply.started": "2021-08-22T03:48:42.216612Z"
        },
        "tags": [],
        "id": "FErY4iBcdQvQ"
      },
      "source": [
        "def reset_model():\n",
        "    net = Net()\n",
        "    net = net.to(device)\n",
        "\n",
        "    # Loss and Optimizer\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
        "    return net,criterion,optimizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FG0NiaT7dQvQ"
      },
      "source": [
        "### L1 Regularization\n",
        "\n",
        "Here, we define a L1 Regularizer and add it to our loss function. The L1 Regularization term basically adds a penalty, equivalent to the absolute value of the magnitude of the weights. This ensures that the magnitude of the weights do not become too high.\n",
        "\n",
        "We have seen in the previous lectures that adding the L1 regularizer ensures sparsity. This is important becuase our ultimate aim is to prune connections from our network. That would mean we should try to make as many weights extremely close to 0 as possible. Thus, adding this penalty term ensures sparsity. The L1 Regularizer is also known as Lass Regression and it can be defined as follows:\n",
        "\n",
        "$$P=\\lambda*\\sum_{i=1}^{n}|\\theta_{i}-0|$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-08-22T03:55:47.891177Z",
          "iopub.status.busy": "2021-08-22T03:55:47.890936Z",
          "iopub.status.idle": "2021-08-22T03:55:47.895102Z",
          "shell.execute_reply": "2021-08-22T03:55:47.894524Z",
          "shell.execute_reply.started": "2021-08-22T03:55:47.891153Z"
        },
        "tags": [],
        "id": "_4m7SZ_IdQvR"
      },
      "source": [
        "def l1_regularizer(net, loss, beta):\n",
        "    l1_crit = nn.L1Loss(size_average=False)\n",
        "    reg_loss = 0\n",
        "    for param in net.parameters():\n",
        "        target = (torch.FloatTensor(param.size()).zero_()).to(device)\n",
        "        reg_loss += l1_crit(param, target)\n",
        "        \n",
        "    loss += beta * reg_loss\n",
        "    return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99v_4m1wdQvT"
      },
      "source": [
        "### Initializing the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-08-22T03:48:44.754283Z",
          "iopub.status.busy": "2021-08-22T03:48:44.753870Z",
          "iopub.status.idle": "2021-08-22T03:48:56.351550Z",
          "shell.execute_reply": "2021-08-22T03:48:56.350763Z",
          "shell.execute_reply.started": "2021-08-22T03:48:44.754257Z"
        },
        "tags": [],
        "id": "slUSwUbLdQvU"
      },
      "source": [
        "net, criterion, optimizer = reset_model()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9OSsmak5dQvV"
      },
      "source": [
        "### Defining the training function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-08-22T03:52:42.257057Z",
          "iopub.status.busy": "2021-08-22T03:52:42.255253Z",
          "iopub.status.idle": "2021-08-22T03:52:42.265357Z",
          "shell.execute_reply": "2021-08-22T03:52:42.264753Z",
          "shell.execute_reply.started": "2021-08-22T03:52:42.257030Z"
        },
        "jupyter": {
          "outputs_hidden": false
        },
        "tags": [],
        "id": "JDWXaFZNdQvV"
      },
      "source": [
        "# Train the Model\n",
        "\n",
        "def training(net, reset = True):\n",
        "    if reset == True:\n",
        "        net, criterion, optimizer = reset_model()\n",
        "    else:\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
        "    \n",
        "    net.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss = 0\n",
        "        accuracy = []\n",
        "        for i, (images, labels) in enumerate(train_loader):\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "            temp_labels = labels\n",
        "\n",
        "            # Forward + Backward + Optimize\n",
        "            optimizer.zero_grad()\n",
        "            outputs = net(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            if use_reg == True :\n",
        "                loss = l1_regularizer(net,loss,beta=0.001)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            correct = (predicted == temp_labels).sum()\n",
        "            accuracy.append(correct/float(batch_size))\n",
        "\n",
        "        print('Epoch: %d, Loss: %.4f, Accuracy: %.4f' %(epoch+1,total_loss, (sum(accuracy)/float(len(accuracy)))))\n",
        "    \n",
        "    return net"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gUhPsP5fdQvW"
      },
      "source": [
        "### Defining the testing function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-08-22T03:52:29.765348Z",
          "iopub.status.busy": "2021-08-22T03:52:29.765101Z",
          "iopub.status.idle": "2021-08-22T03:52:29.769284Z",
          "shell.execute_reply": "2021-08-22T03:52:29.768726Z",
          "shell.execute_reply.started": "2021-08-22T03:52:29.765324Z"
        },
        "jupyter": {
          "outputs_hidden": false
        },
        "tags": [],
        "id": "UrSUcSK6dQvY"
      },
      "source": [
        "# Test the Model\n",
        "def testing(net):\n",
        "    net.eval() \n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for images, labels in test_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = net(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum()\n",
        "\n",
        "    print('Test Accuracy of the network on the 10000 test images: %.2f %%' % (100.0 * correct / total))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0FzbkftUdQvZ"
      },
      "source": [
        "### Training and testing the network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-08-22T03:52:46.698895Z",
          "iopub.status.busy": "2021-08-22T03:52:46.698476Z",
          "iopub.status.idle": "2021-08-22T03:53:32.773647Z",
          "shell.execute_reply": "2021-08-22T03:53:32.773044Z",
          "shell.execute_reply.started": "2021-08-22T03:52:46.698869Z"
        },
        "jupyter": {
          "outputs_hidden": false
        },
        "tags": [],
        "id": "eATiSGSNdQva"
      },
      "source": [
        "reset = True\n",
        "net = training(net, reset)\n",
        "testing(net)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6O9uaphmdQvc"
      },
      "source": [
        "### Function for plotting the weight distribution\n",
        "\n",
        "We want to plot the weights learnt during training and we define the following function to do so. It is important to note that in order ro achieve smoothness in the behaviour of the plot we use 256 clusters here where the nearest weight values are rounded to. Feel free to experiment with defining lesser number of clusters for the weight distribution approximation. However, the plot may not be as smooth. \n",
        "\n",
        "We plot the cluster values in the x-axis and the frequency of the weights in the y-axis."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-08-22T03:53:46.240196Z",
          "iopub.status.busy": "2021-08-22T03:53:46.238715Z",
          "iopub.status.idle": "2021-08-22T03:53:46.247636Z",
          "shell.execute_reply": "2021-08-22T03:53:46.247064Z",
          "shell.execute_reply.started": "2021-08-22T03:53:46.240164Z"
        },
        "tags": [],
        "id": "5JAZDiR_dQvd"
      },
      "source": [
        "def weightdistribution(weights):\n",
        "    maxim= np.amax(weights)\n",
        "    print(\"Maximum value of learnt weights: \" + str(maxim))\n",
        "    \n",
        "    minim= np.amin(weights)\n",
        "    print(\"Minimum value of learnt weights: \" + str(minim))\n",
        "    \n",
        "    step= (maxim-minim)/255\n",
        "    freq= np.zeros(256)\n",
        "    steps=[]\n",
        "\n",
        "    for i in range(0,256):\n",
        "        steps.append(minim)\n",
        "        minim+=step\n",
        "    \n",
        "    m = weights.shape[0]\n",
        "    \n",
        "    for i in range(0,m):\n",
        "        e= weights[i]\n",
        "        dist= (steps-e)**2\n",
        "        freq[np.argmin(dist)]+=1\n",
        "    \n",
        "        \n",
        "    plt.plot(steps,list(freq))\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-08-22T03:53:47.763488Z",
          "iopub.status.busy": "2021-08-22T03:53:47.763056Z",
          "iopub.status.idle": "2021-08-22T03:54:01.753719Z",
          "shell.execute_reply": "2021-08-22T03:54:01.753125Z",
          "shell.execute_reply.started": "2021-08-22T03:53:47.763462Z"
        },
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "mrTIZqXJdQve"
      },
      "source": [
        "weightdistribution(net.state_dict()['layer2.0.weight'].cpu().numpy().flatten())\n",
        "weightdistribution(net.state_dict()['fc1.weight'].cpu().numpy().flatten())\n",
        "weightdistribution(net.state_dict()['fc2.weight'].cpu().numpy().flatten())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Gf05jlfdQvf"
      },
      "source": [
        "### Defining the pruning function\n",
        "\n",
        "We use a threshold based criteria to remove certain connections in the networks which fall within a threshold value. Hence, we define a function which takes the network as input and a vlaue for threshold and forces those connections to be 0 which fall with the threshold given by the following equation.\n",
        "\n",
        "$$\n",
        "\\begin{equation}\n",
        " W_{i} = \n",
        " \\begin{cases} \n",
        "      W_{i} & W_{i} < -\\theta \\\\\n",
        "      0 & -\\theta\\leq W_{i}\\leq \\theta \\\\\n",
        "      W_{i} & \\theta < W_{i} \n",
        "  \\end{cases}\n",
        "\\end{equation}\n",
        "$$\n",
        "\n",
        "We prune both the weights and biases of Convolution layers, Fully Connected layers and the Batch Normalization layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-08-22T03:54:01.758878Z",
          "iopub.status.busy": "2021-08-22T03:54:01.757112Z",
          "iopub.status.idle": "2021-08-22T03:54:01.837762Z",
          "shell.execute_reply": "2021-08-22T03:54:01.837207Z",
          "shell.execute_reply.started": "2021-08-22T03:54:01.758849Z"
        },
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "wU6cMAHKdQvg"
      },
      "source": [
        "def prune_weight(net, threshold):\n",
        "    \n",
        "    for m in net.modules():\n",
        "        if isinstance(m,nn.Conv2d) or isinstance(m,nn.BatchNorm2d) or isinstance(m,nn.Linear):\n",
        "\n",
        "            temp_weight = m.weight.data.cpu().numpy()\n",
        "            dims = temp_weight.shape\n",
        "            print('WEIGHT ',dims)\n",
        "            print(\"Before pruning------->\")\n",
        "            print(np.count_nonzero(temp_weight))\n",
        "\n",
        "            temp_weight = temp_weight.flatten()\n",
        "\n",
        "            [x1,x2]=((np.where(np.all([[(-1*threshold) < (temp_weight)] , [(temp_weight) < threshold]],axis=0))))\n",
        "            temp_weight[x2] = 0\n",
        "            temp_weight = np.reshape(temp_weight,dims)\n",
        "            print(\"After pruning------->\")\n",
        "            print(np.count_nonzero(temp_weight))\n",
        "            print('-------------------------------------------------------------------------------')\n",
        "            m.weight.data = (torch.FloatTensor(temp_weight).to(device))\n",
        "\n",
        "            temp_bias = m.bias.data.cpu().numpy()\n",
        "            dims = temp_bias.shape\n",
        "            print('BIAS ',dims)\n",
        "            print(\"Before pruning------->\")\n",
        "            print(np.count_nonzero(temp_bias))\n",
        "\n",
        "            temp_bias = temp_bias.flatten()\n",
        "\n",
        "            [x1,x2]=((np.where(np.all([[(-1*threshold) < (temp_bias)] , [(temp_bias) < threshold]],axis=0))))\n",
        "            temp_bias[x2] = 0\n",
        "            temp_bias = np.reshape(temp_bias,dims)\n",
        "            print(\"After pruning------->\")\n",
        "            print(np.count_nonzero(temp_bias))\n",
        "            print('-------------------------------------------------------------------------------')\n",
        "            m.bias.data = (torch.FloatTensor(temp_bias).to(device))\n",
        "\n",
        "# for m in net.modules():\n",
        "#     if isinstance(m,nn.Conv2d):\n",
        "#         print m.bias.data\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OHKp57TBdQvi"
      },
      "source": [
        "Here, we check how many connetions have been pruned"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-08-22T03:54:01.843487Z",
          "iopub.status.busy": "2021-08-22T03:54:01.841616Z",
          "iopub.status.idle": "2021-08-22T03:54:01.878499Z",
          "shell.execute_reply": "2021-08-22T03:54:01.878029Z",
          "shell.execute_reply.started": "2021-08-22T03:54:01.843460Z"
        },
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "iLyN7CEhdQvj"
      },
      "source": [
        "threshold = 0.01\n",
        "prune_weight(net,threshold)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ybzQtfBBdQvk"
      },
      "source": [
        "Training the network again using the regularizer. The loss is modified as \n",
        "\n",
        "$$\n",
        "loss = loss + regularized_loss\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-08-22T03:55:57.379938Z",
          "iopub.status.busy": "2021-08-22T03:55:57.379723Z",
          "iopub.status.idle": "2021-08-22T03:56:55.745158Z",
          "shell.execute_reply": "2021-08-22T03:56:55.744523Z",
          "shell.execute_reply.started": "2021-08-22T03:55:57.379914Z"
        },
        "jupyter": {
          "outputs_hidden": false
        },
        "tags": [],
        "id": "MsePIAEVdQvl"
      },
      "source": [
        "reset = True\n",
        "use_reg = True\n",
        "net = training(net, reset)\n",
        "testing(net)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "azfv603SdQvl"
      },
      "source": [
        "<b>Visualizing the weight distributions with the regularized loss</b>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-08-22T03:57:09.129806Z",
          "iopub.status.busy": "2021-08-22T03:57:09.129387Z",
          "iopub.status.idle": "2021-08-22T03:57:23.074341Z",
          "shell.execute_reply": "2021-08-22T03:57:23.073829Z",
          "shell.execute_reply.started": "2021-08-22T03:57:09.129777Z"
        },
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "r5H8IWgIdQvm"
      },
      "source": [
        "weightdistribution(net.state_dict()['layer2.0.weight'].cpu().numpy().flatten())\n",
        "weightdistribution(net.state_dict()['fc1.weight'].cpu().numpy().flatten())\n",
        "weightdistribution(net.state_dict()['fc2.weight'].cpu().numpy().flatten())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tMxSCtfPdQvo"
      },
      "source": [
        "By using the L1 Regularizer, we see that we have forced the network to learn weights within a constrained subspace. Since more number of weights are closer to 0, it is evident that we will be able to make more weights 0 by using the same threshold as before. This, therefore ensures sparsity in every layer of the network.\n",
        "\n",
        "An interesting observation is that the netowrk tells us that it does not require biases for certain layers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-08-22T03:57:26.821944Z",
          "iopub.status.busy": "2021-08-22T03:57:26.819888Z",
          "iopub.status.idle": "2021-08-22T03:57:26.860758Z",
          "shell.execute_reply": "2021-08-22T03:57:26.860262Z",
          "shell.execute_reply.started": "2021-08-22T03:57:26.821915Z"
        },
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "ql1J9f6XdQvp"
      },
      "source": [
        "threshold = 0.01\n",
        "prune_weight(net,threshold)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UqeEM8cgdQvq"
      },
      "source": [
        "### References\n",
        "\n",
        "1. http://www.pyimagesearch.com/2016/09/19/understanding-regularization-for-image-classification-and-machine-learning/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        },
        "id": "YFaKZv7ldQvr"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}