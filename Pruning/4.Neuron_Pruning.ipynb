{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "4.Neuron_Pruning.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ektagavas/CVSummerSchool2021/blob/main/Pruning/4.Neuron_Pruning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sxTF4W9tiU-S"
      },
      "source": [
        "## Pruning Neurons in Neural Networks\n",
        "\n",
        "Throughout the summer school, we have been discussing big neural networks models with a huge amount of parameters. The concentration of the parameters is much more in the fully connected layers and this brings us to the question whether so many parameters in the FC Layers are necessary.\n",
        "\n",
        "<img src='images/pruning.jpg', style=\"width: 500px; height: 250px\">\n",
        "\n",
        "While pruning connections of the network by making the weight space sparse, we were able to drop connections. However, the model occupied the same amount of size as before in memory while doing a forward pass. Thus we would not be able to exploit that procedure to its fullest unless we write separate modules for sparse matrix multiplications in neural networks which the deep learning libraries do not provide.\n",
        "\n",
        "In this notebook we shall see one of the solutions which remove redundant neurons from the network. Our focus shall be on fully connected layers. Through this approach we can not only reduce the size of the network on disk but the resultant model as whole will occupy less space in the RAM during a forward pass."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-08-22T04:20:59.545750Z",
          "iopub.status.busy": "2021-08-22T04:20:59.545226Z",
          "iopub.status.idle": "2021-08-22T04:21:00.822946Z",
          "shell.execute_reply": "2021-08-22T04:21:00.822423Z",
          "shell.execute_reply.started": "2021-08-22T04:20:59.545684Z"
        },
        "id": "lxwOj5x6iU-Y"
      },
      "source": [
        "import numpy as np\n",
        "np.random.seed(1337)  # for reproducibility\n",
        "import torch \n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.datasets as dsets\n",
        "import torchvision.transforms as transforms\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Frl3WB1OiU-a"
      },
      "source": [
        "### Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-08-22T04:21:00.827341Z",
          "iopub.status.busy": "2021-08-22T04:21:00.825896Z",
          "iopub.status.idle": "2021-08-22T04:21:01.416172Z",
          "shell.execute_reply": "2021-08-22T04:21:01.415514Z",
          "shell.execute_reply.started": "2021-08-22T04:21:00.827314Z"
        },
        "id": "suYNgIb-iU-b"
      },
      "source": [
        "num_epochs = 5\n",
        "batch_size = 100\n",
        "learning_rate = 0.001\n",
        "use_reg = False\n",
        "alpha = 0.5\n",
        "beta = 1-alpha\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using {device} device\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jPydZXdXiU-e"
      },
      "source": [
        "### Downloading MNIST"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-08-22T04:21:01.417556Z",
          "iopub.status.busy": "2021-08-22T04:21:01.417380Z",
          "iopub.status.idle": "2021-08-22T04:22:45.925962Z",
          "shell.execute_reply": "2021-08-22T04:22:45.925373Z",
          "shell.execute_reply.started": "2021-08-22T04:21:01.417534Z"
        },
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "D8BWuCFbiU-f"
      },
      "source": [
        "train_dataset = dsets.MNIST(root='../../data/lab6',\n",
        "                            train=True, \n",
        "                            transform=transforms.ToTensor(),\n",
        "                            download=True)\n",
        "\n",
        "test_dataset = dsets.MNIST(root='../../data/lab6',\n",
        "                           train=False, \n",
        "                           transform=transforms.ToTensor())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qvxwA_1JiU-g"
      },
      "source": [
        "### Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-08-22T04:22:45.930238Z",
          "iopub.status.busy": "2021-08-22T04:22:45.928813Z",
          "iopub.status.idle": "2021-08-22T04:22:45.935371Z",
          "shell.execute_reply": "2021-08-22T04:22:45.934774Z",
          "shell.execute_reply.started": "2021-08-22T04:22:45.930200Z"
        },
        "id": "b6bF-FuZiU-h"
      },
      "source": [
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
        "                                           batch_size=batch_size, \n",
        "                                           shuffle=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
        "                                          batch_size=batch_size, \n",
        "                                          shuffle=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xnsQWc_YiU-j"
      },
      "source": [
        "### Defining a fully connected network to classify MNIST"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-08-22T04:22:45.939598Z",
          "iopub.status.busy": "2021-08-22T04:22:45.938251Z",
          "iopub.status.idle": "2021-08-22T04:22:45.945373Z",
          "shell.execute_reply": "2021-08-22T04:22:45.944810Z",
          "shell.execute_reply.started": "2021-08-22T04:22:45.939572Z"
        },
        "id": "bP2jSkAniU-k"
      },
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(784, 2000)\n",
        "        self.fc2 = nn.Linear(2000, 1000)\n",
        "        self.fc3 = nn.Linear(1000, 10)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        out = x.view(x.size(0), -1)\n",
        "        out = self.fc1(out)\n",
        "        out = self.fc2(F.relu(out))\n",
        "        out = self.fc3(F.relu(out))\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nIPwBzwhiU-l"
      },
      "source": [
        "<b>The below function is called to reinitialize the weights of the network and define the required loss criterion and the optimizer.</b> "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-08-22T04:22:45.949995Z",
          "iopub.status.busy": "2021-08-22T04:22:45.948232Z",
          "iopub.status.idle": "2021-08-22T04:22:45.954559Z",
          "shell.execute_reply": "2021-08-22T04:22:45.953997Z",
          "shell.execute_reply.started": "2021-08-22T04:22:45.949965Z"
        },
        "id": "aRUt3CZ3iU-m"
      },
      "source": [
        "def reset_model():\n",
        "    net = Net()\n",
        "    net = net.to(device)\n",
        "\n",
        "    # Loss and Optimizer\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
        "    return net,criterion,optimizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XIxt8HpkiU-n"
      },
      "source": [
        "### Network Initialization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-08-22T04:22:45.958690Z",
          "iopub.status.busy": "2021-08-22T04:22:45.957346Z",
          "iopub.status.idle": "2021-08-22T04:22:48.218486Z",
          "shell.execute_reply": "2021-08-22T04:22:48.217956Z",
          "shell.execute_reply.started": "2021-08-22T04:22:45.958664Z"
        },
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "GfunRnmhiU-o"
      },
      "source": [
        "net, criterion, optimizer = reset_model()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dYWm-U3jiU-p"
      },
      "source": [
        "### Regularizers\n",
        "\n",
        "The goal of introducing l1 regulariser and l2 regulariser is to penalise the connections' weights between neurons to prevent overfitting. However, the application of such regularisers alone in deep neural network are not as successful as in linear regression and logistic regression. On the other hand, in the hardware computation especially using GPU, dropping connections may not save computation time and memory unless some special coding and processing is used. The introduction of dropout achieve great success to avoid over-fitting in practice with these two regularisers. These regularisation techniques are suitable for preventing overfitting but may not be helpful in simplifying the NN structure. \n",
        "\n",
        "Here, we see two more regularizers which help is exploring the sparsity.\n",
        "\n",
        "$$\n",
        "li\\_regularizer = \\lambda_{l_{i}}\\sum_{l=1}^{L}\\sum_{j=1}^{n_{l}}\\sqrt{\\sum_{i=1}^{n_{l-1}}(W_{i,j}^{l})^{2}}\n",
        "$$\n",
        "\n",
        "$$\n",
        "lo\\_regularizer = \\lambda_{l_{i}}\\sum_{l=1}^{L}\\sum_{i=1}^{n_{l-1}}\\sqrt{\\sum_{j=1}^{n_{l}}(W_{i,j}^{l})^{2}}\n",
        "$$\n",
        "\n",
        "$$\n",
        "loss = loss + \\alpha*li\\_regularizer + (1-\\alpha)*lo\\_regularizer\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XPRBznNciU-q"
      },
      "source": [
        "### LI Regularizer definition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-08-22T04:22:48.223527Z",
          "iopub.status.busy": "2021-08-22T04:22:48.222308Z",
          "iopub.status.idle": "2021-08-22T04:22:48.228086Z",
          "shell.execute_reply": "2021-08-22T04:22:48.227546Z",
          "shell.execute_reply.started": "2021-08-22T04:22:48.223503Z"
        },
        "id": "c80lwcfjiU-r"
      },
      "source": [
        "def li_regularizer(net):\n",
        "    \n",
        "    lambda_li = 0.001\n",
        "    li_reg_loss = 0\n",
        "    for m in net.modules():\n",
        "        if isinstance(m,nn.Linear):\n",
        "            temp_loss = torch.sum(((torch.sum(((m.weight)**2),1))**0.5),0)\n",
        "            li_reg_loss += lambda_li*temp_loss\n",
        "\n",
        "    return li_reg_loss\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LbgN-PwCiU-s"
      },
      "source": [
        "### LO Regularizer definition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-08-22T05:55:23.982201Z",
          "iopub.status.busy": "2021-08-22T05:55:23.982005Z",
          "iopub.status.idle": "2021-08-22T05:55:23.985531Z",
          "shell.execute_reply": "2021-08-22T05:55:23.984961Z",
          "shell.execute_reply.started": "2021-08-22T05:55:23.982180Z"
        },
        "tags": [],
        "id": "0uomH3WuiU-t"
      },
      "source": [
        "def lo_regularizer(net):\n",
        "    \n",
        "\tlambda_lo = 0.001\n",
        "\tlo_reg_loss = 0\n",
        "\tfor m in net.modules():\n",
        "\t\tif isinstance(m,nn.Linear):\n",
        "\t#             temp_loss = torch.sum(((torch.sum(((m.weight)**2),0))**0.5),1)\n",
        "\t\t\ttemp_loss = ((m.weight**2).sum(0)**0.5).sum(0)\n",
        "\t\t\tlo_reg_loss += lambda_lo*temp_loss\n",
        "\n",
        "\treturn lo_reg_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vYEa3hfiiU-t"
      },
      "source": [
        "### L1 Regularizer definition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-08-22T04:22:48.241389Z",
          "iopub.status.busy": "2021-08-22T04:22:48.239950Z",
          "iopub.status.idle": "2021-08-22T04:22:48.246554Z",
          "shell.execute_reply": "2021-08-22T04:22:48.246068Z",
          "shell.execute_reply.started": "2021-08-22T04:22:48.241362Z"
        },
        "id": "k55nfY8SiU-u"
      },
      "source": [
        "def l1_regularizer(net, loss, beta):\n",
        "    l1_crit = nn.L1Loss(size_average=False)\n",
        "    reg_loss = 0\n",
        "    for param in net.parameters():\n",
        "        target = (torch.FloatTensor(param.size()).zero_()).to(device)\n",
        "        reg_loss += l1_crit(param, target)\n",
        "        print(type(reg_loss))\n",
        "        \n",
        "    loss += beta * reg_loss\n",
        "    return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FWwPKM_IiU-v"
      },
      "source": [
        "### Training and Testing functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-08-22T04:22:48.250562Z",
          "iopub.status.busy": "2021-08-22T04:22:48.249176Z",
          "iopub.status.idle": "2021-08-22T04:22:48.257325Z",
          "shell.execute_reply": "2021-08-22T04:22:48.256649Z",
          "shell.execute_reply.started": "2021-08-22T04:22:48.250536Z"
        },
        "id": "RQGBp1U9iU-v"
      },
      "source": [
        "# Train the Model\n",
        "\n",
        "def training(net):\n",
        "    \n",
        "    net, criterion, optimizer = reset_model()\n",
        "    \n",
        "    net.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss = 0\n",
        "        accuracy = []\n",
        "        for i, (images, labels) in enumerate(train_loader):\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "            temp_labels = labels\n",
        "\n",
        "            # Forward + Backward + Optimize\n",
        "            optimizer.zero_grad()\n",
        "            outputs = net(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            \n",
        "            if use_reg == True :\n",
        "        \n",
        "                loss = loss + alpha*li_regularizer(net) + beta*lo_regularizer(net)\n",
        "                \n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            correct = (predicted == temp_labels).sum()\n",
        "            accuracy.append(correct/float(batch_size))\n",
        "\n",
        "        print('Epoch: %d, Loss: %.4f, Accuracy: %.4f' %(epoch+1,total_loss, (sum(accuracy)/float(len(accuracy)))))\n",
        "    \n",
        "    return net"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-08-22T04:22:48.261392Z",
          "iopub.status.busy": "2021-08-22T04:22:48.260046Z",
          "iopub.status.idle": "2021-08-22T04:22:48.266711Z",
          "shell.execute_reply": "2021-08-22T04:22:48.266258Z",
          "shell.execute_reply.started": "2021-08-22T04:22:48.261367Z"
        },
        "id": "RqGIt2IBiU-x"
      },
      "source": [
        "# Test the Model\n",
        "def testing(net):\n",
        "    net.eval() \n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for images, labels in test_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = net(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum()\n",
        "\n",
        "    print('Test Accuracy of the network on the 10000 test images: %.2f %%' % (100.0 * correct / total))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-08-22T04:22:48.270709Z",
          "iopub.status.busy": "2021-08-22T04:22:48.269297Z",
          "iopub.status.idle": "2021-08-22T04:23:25.675624Z",
          "shell.execute_reply": "2021-08-22T04:23:25.675017Z",
          "shell.execute_reply.started": "2021-08-22T04:22:48.270683Z"
        },
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "aOV5f0IFiU-x"
      },
      "source": [
        "use_reg = False\n",
        "net = training(net)\n",
        "testing(net)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AEAfc7vjiU-y"
      },
      "source": [
        "### Visualizing the weight Space"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-08-22T04:23:25.679619Z",
          "iopub.status.busy": "2021-08-22T04:23:25.678324Z",
          "iopub.status.idle": "2021-08-22T04:23:25.685527Z",
          "shell.execute_reply": "2021-08-22T04:23:25.685022Z",
          "shell.execute_reply.started": "2021-08-22T04:23:25.679591Z"
        },
        "id": "cYR__-4GiU-z"
      },
      "source": [
        "def weightdistribution(weights):\n",
        "    maxim= np.amax(weights)\n",
        "    print(\"Maximum value of learnt weights: \" + str(maxim))\n",
        "    \n",
        "    minim= np.amin(weights)\n",
        "    print(\"Minimum value of learnt weights: \" + str(minim))\n",
        "    \n",
        "    step= (maxim-minim)/255\n",
        "    freq= np.zeros(256)\n",
        "    steps=[]\n",
        "\n",
        "    for i in range(0,256):\n",
        "        steps.append(minim)\n",
        "        minim+=step\n",
        "    \n",
        "    m = weights.shape[0]\n",
        "    \n",
        "    for i in range(0,m):\n",
        "        e= weights[i]\n",
        "        dist= (steps-e)**2\n",
        "        freq[np.argmin(dist)]+=1\n",
        "    \n",
        "        \n",
        "    plt.plot(steps,list(freq))\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-08-22T04:23:25.689911Z",
          "iopub.status.busy": "2021-08-22T04:23:25.688264Z",
          "iopub.status.idle": "2021-08-22T04:25:07.965076Z",
          "shell.execute_reply": "2021-08-22T04:25:07.964494Z",
          "shell.execute_reply.started": "2021-08-22T04:23:25.689885Z"
        },
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "5g5OTSu0iU-z"
      },
      "source": [
        "weightdistribution(net.state_dict()['fc1.weight'].cpu().numpy().flatten())\n",
        "weightdistribution(net.state_dict()['fc2.weight'].cpu().numpy().flatten())\n",
        "weightdistribution(net.state_dict()['fc3.weight'].cpu().numpy().flatten())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5WVMV7ReiU-1"
      },
      "source": [
        "### Training with the defined regularizers\n",
        "\n",
        "Next we train with the li and lo regularizers defined previously. We hope to address the issue of overfitting to some extent by using these regularizers. Also, the weight matrices are plotted to observe the distribution. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-08-22T05:55:30.109145Z",
          "iopub.status.busy": "2021-08-22T05:55:30.108887Z",
          "iopub.status.idle": "2021-08-22T05:56:09.403253Z",
          "shell.execute_reply": "2021-08-22T05:56:09.402738Z",
          "shell.execute_reply.started": "2021-08-22T05:55:30.109121Z"
        },
        "jupyter": {
          "outputs_hidden": false
        },
        "tags": [],
        "id": "MkszPEa3iU-1"
      },
      "source": [
        "use_reg = True\n",
        "net = training(net)\n",
        "testing(net)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-08-22T05:56:12.593796Z",
          "iopub.status.busy": "2021-08-22T05:56:12.592285Z",
          "iopub.status.idle": "2021-08-22T05:57:44.086326Z",
          "shell.execute_reply": "2021-08-22T05:57:44.085852Z",
          "shell.execute_reply.started": "2021-08-22T05:56:12.593768Z"
        },
        "jupyter": {
          "outputs_hidden": false
        },
        "tags": [],
        "id": "M0JiGobviU-2"
      },
      "source": [
        "weightdistribution(net.state_dict()['fc1.weight'].cpu().numpy().flatten())\n",
        "weightdistribution(net.state_dict()['fc2.weight'].cpu().numpy().flatten())\n",
        "weightdistribution(net.state_dict()['fc3.weight'].cpu().numpy().flatten())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1e3MFztRiU-3"
      },
      "source": [
        "### Pruning\n",
        "\n",
        "We next prune the parameters based ona threshold criterion as seen previously. The parameters before and after pruning are printed below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-08-22T05:58:01.587656Z",
          "iopub.status.busy": "2021-08-22T05:58:01.587276Z",
          "iopub.status.idle": "2021-08-22T05:58:01.708335Z",
          "shell.execute_reply": "2021-08-22T05:58:01.707856Z",
          "shell.execute_reply.started": "2021-08-22T05:58:01.587630Z"
        },
        "jupyter": {
          "outputs_hidden": false
        },
        "tags": [],
        "id": "v46wct2KiU-4"
      },
      "source": [
        "threshold = 0.01\n",
        "\n",
        "for m in net.modules():\n",
        "    if isinstance(m,nn.Conv2d) or isinstance(m,nn.BatchNorm2d) or isinstance(m,nn.Linear):\n",
        "        \n",
        "        temp_weight = m.weight.data.cpu().numpy()\n",
        "        dims = temp_weight.shape\n",
        "        print('WEIGHT ',dims)\n",
        "        print(\"Before pruning------->\")\n",
        "        print(np.count_nonzero(temp_weight))\n",
        "        \n",
        "        temp_weight = temp_weight.flatten()\n",
        "        \n",
        "        [x1,x2]=((np.where(np.all([[(-1*threshold) < (temp_weight)] , [(temp_weight) < threshold]],axis=0))))\n",
        "        temp_weight[x2] = 0\n",
        "        temp_weight = np.reshape(temp_weight,dims)\n",
        "        print(\"After pruning------->\")\n",
        "        print(np.count_nonzero(temp_weight))\n",
        "        print('-------------------------------------------------------------------------------')\n",
        "        m.weight.data = (torch.FloatTensor(temp_weight).to(device))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PMvWOVEaiU-7"
      },
      "source": [
        "### Removing all zero rows and columns\n",
        "\n",
        "1. We remove those columns in the weight matrices which have all the values as zero.\n",
        "2. In the weight matrix for the next layer, we need to remove the corresponding rows.\n",
        "3. We must eventually end up removing the intersection of the two.\n",
        "\n",
        "<img src='https://github.com/Ektagavas/CVSummerSchool2021/blob/main/Pruning/images/drop_neuron.png?raw=1'>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-08-22T05:58:06.276381Z",
          "iopub.status.busy": "2021-08-22T05:58:06.275003Z",
          "iopub.status.idle": "2021-08-22T05:58:06.293232Z",
          "shell.execute_reply": "2021-08-22T05:58:06.292773Z",
          "shell.execute_reply.started": "2021-08-22T05:58:06.276357Z"
        },
        "jupyter": {
          "outputs_hidden": false
        },
        "tags": [],
        "id": "l3tmmjcDiU-8"
      },
      "source": [
        "fc1_weight = net.fc1.weight.data.cpu().numpy()\n",
        "fc2_weight = net.fc2.weight.data.cpu().numpy()\n",
        "fc3_weight = net.fc3.weight.data.cpu().numpy()\n",
        "\n",
        "print('---------------------------Original dimensions-----------------------')\n",
        "print(fc1_weight.shape)\n",
        "print(fc2_weight.shape)\n",
        "print(fc3_weight.shape)\n",
        "print('---------------------------Final dimensions-----------------------')\n",
        "\n",
        "fc1_row_zero = np.where(~fc1_weight.any(axis=1))[0]\n",
        "fc2_col_zero = np.where(~fc2_weight.any(axis=0))[0]\n",
        "indices = set(fc1_row_zero).intersection(fc2_col_zero)\n",
        "indices = list(indices)\n",
        "\n",
        "fc1_weight = np.delete(fc1_weight,indices,axis=0)\n",
        "fc2_weight = np.delete(fc2_weight,indices,axis=1)\n",
        "\n",
        "\n",
        "fc2_row_zero = np.where(~fc2_weight.any(axis=1))[0]\n",
        "fc3_col_zero = np.where(~fc3_weight.any(axis=0))[0]\n",
        "indices = set(fc2_row_zero).intersection(fc3_col_zero)\n",
        "indices = list(indices)\n",
        "\n",
        "fc2_weight = np.delete(fc2_weight,indices,axis=0)\n",
        "fc3_weight = np.delete(fc3_weight,indices,axis=1)\n",
        "\n",
        "print(fc1_weight.shape)\n",
        "print(fc2_weight.shape)\n",
        "print(fc3_weight.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "igdgq650iU--"
      },
      "source": [
        "### Define a Modified Network\n",
        "\n",
        "After removing entire rows and columns of the bigger weight matrices we are left with smaller number of parameters which we can use to initialize a smaller network. In order to do that, first we need to define one.\n",
        "\n",
        "We define a modified network by using the dimensions of the modified weight matrix which we received in our previous step. Then, we reset that model too."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-08-22T05:58:16.675784Z",
          "iopub.status.busy": "2021-08-22T05:58:16.674348Z",
          "iopub.status.idle": "2021-08-22T05:58:16.681043Z",
          "shell.execute_reply": "2021-08-22T05:58:16.680566Z",
          "shell.execute_reply.started": "2021-08-22T05:58:16.675758Z"
        },
        "jupyter": {
          "outputs_hidden": false
        },
        "tags": [],
        "id": "282sZvzjiU--"
      },
      "source": [
        "class Mod_Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Mod_Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(fc1_weight.shape[1], fc1_weight.shape[0])\n",
        "        self.fc2 = nn.Linear(fc2_weight.shape[1], fc2_weight.shape[0])\n",
        "        self.fc3 = nn.Linear(fc3_weight.shape[1], fc3_weight.shape[0])\n",
        "        \n",
        "    def forward(self, x):\n",
        "        out = x.view(x.size(0), -1)\n",
        "        out = self.fc1(out)\n",
        "        out = self.fc2(F.relu(out))\n",
        "        out = self.fc3(F.relu(out))\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-08-22T05:58:18.043808Z",
          "iopub.status.busy": "2021-08-22T05:58:18.042464Z",
          "iopub.status.idle": "2021-08-22T05:58:18.047825Z",
          "shell.execute_reply": "2021-08-22T05:58:18.047326Z",
          "shell.execute_reply.started": "2021-08-22T05:58:18.043783Z"
        },
        "jupyter": {
          "outputs_hidden": false
        },
        "tags": [],
        "id": "v36cw_XNiU-_"
      },
      "source": [
        "def reset_mod_model():\n",
        "    modnet = Mod_Net()\n",
        "    modnet = modnet.to(device)\n",
        "\n",
        "    # Loss and Optimizer\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(modnet.parameters(), lr=learning_rate)\n",
        "    return modnet,criterion,optimizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-08-22T05:58:18.934518Z",
          "iopub.status.busy": "2021-08-22T05:58:18.933140Z",
          "iopub.status.idle": "2021-08-22T05:58:18.940658Z",
          "shell.execute_reply": "2021-08-22T05:58:18.940159Z",
          "shell.execute_reply.started": "2021-08-22T05:58:18.934493Z"
        },
        "tags": [],
        "id": "58F-vdmRiU_A"
      },
      "source": [
        "modnet, criterion, optimizer = reset_mod_model()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0OGkCzBMiU_B"
      },
      "source": [
        "Here we initialize the smaller model with the smaller weight matrices. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-08-22T05:58:21.661531Z",
          "iopub.status.busy": "2021-08-22T05:58:21.660095Z",
          "iopub.status.idle": "2021-08-22T05:58:21.666208Z",
          "shell.execute_reply": "2021-08-22T05:58:21.665693Z",
          "shell.execute_reply.started": "2021-08-22T05:58:21.661507Z"
        },
        "jupyter": {
          "outputs_hidden": false
        },
        "tags": [],
        "id": "pWkl43AHiU_C"
      },
      "source": [
        "modnet.fc1.weight.data = (torch.FloatTensor(fc1_weight).to(device))\n",
        "modnet.fc2.weight.data = (torch.FloatTensor(fc2_weight).to(device))\n",
        "modnet.fc3.weight.data = (torch.FloatTensor(fc3_weight).to(device))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-08-22T05:58:23.651462Z",
          "iopub.status.busy": "2021-08-22T05:58:23.651101Z",
          "iopub.status.idle": "2021-08-22T05:58:24.603666Z",
          "shell.execute_reply": "2021-08-22T05:58:24.603197Z",
          "shell.execute_reply.started": "2021-08-22T05:58:23.651438Z"
        },
        "jupyter": {
          "outputs_hidden": false
        },
        "tags": [],
        "id": "Rc-v0x8fiU_C"
      },
      "source": [
        "testing(modnet)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i5jffZ5aiU_E"
      },
      "source": [
        "### Excercise\n",
        "\n",
        "1. Retrain the smaller model and check if the performance improves.\n",
        "\n",
        "2. Using the l1 regularizer along with lo and li with further enhance sparsity. This might result in a even smaller model. Try it out!!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        },
        "id": "6rl4HhV5iU_E"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h6PLBxMMiU_F"
      },
      "source": [
        "### References\n",
        "\n",
        "1. https://arxiv.org/pdf/1606.07326.pdf"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        },
        "id": "AZhkQC1TiU_F"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}