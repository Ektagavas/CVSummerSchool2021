{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "fetal_head_segmentation.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TdnDOtCa5KEQ"
      },
      "source": [
        "# Segmentation of fetal head in ultrasound data\n",
        "---\n",
        "The US dataset used for this task can be found at [HC-18 challenge](https://hc18.grand-challenge.org/).\n",
        "\n",
        "## Obtaining data & setting up requirements"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_j1yXasiZT3E",
        "outputId": "09ec32ab-cd07-40c8-93ca-98ce10280596"
      },
      "source": [
        "# Connect data\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vqUD4FC7jvDL"
      },
      "source": [
        "! pwd\n",
        "! mkdir us_dataset\n",
        "! tar -xvzf /content/gdrive/MyDrive/us_data.tar.gz -C us_dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VHtyKjs_l2Ay",
        "outputId": "ce96ef6c-e343-41a7-bb47-865290c08bf3"
      },
      "source": [
        "! ls us_dataset"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "all_images  all_masks\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yzsg3-ajdUwO",
        "outputId": "07e1148b-5e4f-4bb4-9d2b-ef0d53f89c64"
      },
      "source": [
        "# Obtain U-Net code by Meet Shah\n",
        "! git clone https://github.com/meetps/pytorch-semseg"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'pytorch-semseg'...\n",
            "remote: Enumerating objects: 1088, done.\u001b[K\n",
            "remote: Total 1088 (delta 0), reused 0 (delta 0), pack-reused 1088\u001b[K\n",
            "Receiving objects: 100% (1088/1088), 277.47 KiB | 2.69 MiB/s, done.\n",
            "Resolving deltas: 100% (738/738), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J265SXNRfWg8"
      },
      "source": [
        "# rearrange files for easy access\n",
        "! mv pytorch-semseg/ptsemseg ."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kwrTSz2ke-1t",
        "outputId": "55a33f70-8471-4e4d-d88f-9fa5ef5ed2c2"
      },
      "source": [
        "# Install kornia, a useful library that does a lot of operations on PyTorch tensors\n",
        "! pip install kornia"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting kornia\n",
            "  Downloading kornia-0.5.8-py2.py3-none-any.whl (303 kB)\n",
            "\u001b[?25l\r\u001b[K     |█                               | 10 kB 19.7 MB/s eta 0:00:01\r\u001b[K     |██▏                             | 20 kB 25.6 MB/s eta 0:00:01\r\u001b[K     |███▎                            | 30 kB 13.2 MB/s eta 0:00:01\r\u001b[K     |████▎                           | 40 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 51 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 61 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 71 kB 6.0 MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 81 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 92 kB 6.9 MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 102 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 112 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 122 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 133 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 143 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 153 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 163 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 174 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 184 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 194 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 204 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 215 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 225 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 235 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 245 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 256 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 266 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 276 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 286 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 296 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 303 kB 5.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from kornia) (1.9.0+cu102)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6.0->kornia) (3.7.4.3)\n",
            "Installing collected packages: kornia\n",
            "Successfully installed kornia-0.5.8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_syGMRhgov9x"
      },
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim import Adam\n",
        "from tqdm import trange\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from kornia.losses import dice_loss\n",
        "from kornia.utils import one_hot\n",
        "from ptsemseg.models.unet import unet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5npcVLQEl-aJ"
      },
      "source": [
        "## Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x0lSQbSAlFlk"
      },
      "source": [
        "class ImageLoader(Dataset):\n",
        "  \"\"\" Data loader class \"\"\"\n",
        "  def __init__(self, path, file_list, aug_list=None, aug_prob=None):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      path (str): path where images stored\n",
        "      file_list (List[str]): list of images in current split\n",
        "      aug_list (List[str]): list of torchvision transforms\n",
        "      aug_prob (float): Probability of applying random aug (if aug_list != None)\n",
        "    \"\"\"\n",
        "    self.path = path\n",
        "    self.file_list = file_list\n",
        "    self.aug_list = aug_list\n",
        "    self.aug_prob = aug_prob\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.file_list)\n",
        "  \n",
        "  def __getitem__(self, idx):\n",
        "    \"\"\" Preprocess and return a single sample & label \"\"\"\n",
        "    img_name = os.path.join(self.path, 'all_images', self.file_list[idx])\n",
        "    mask_fname = self.file_list[idx].split('.')[0] + '_mask.png'\n",
        "    mask_name = os.path.join(self.path, 'all_masks', mask_fname)\n",
        "    img = Image.open(img_name)\n",
        "    mask = Image.open(mask_name)\n",
        "    # Resize to dimensions supported by Vanilla UNet\n",
        "    img = img.resize((572, 572), Image.LANCZOS)\n",
        "    mask = mask.resize((388, 388), Image.NEAREST)\n",
        "\n",
        "\n",
        "    img = np.array(img)\n",
        "    mask = np.array(mask)\n",
        "    mask[mask == 255] = 1\n",
        "\n",
        "    mask = torch.Tensor([mask])\n",
        "\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)  \n",
        "    img = torch.Tensor(img)\n",
        "    img = img.permute(2, 0, 1)\n",
        "\n",
        "    # select and apply random augmentation (if passed)\n",
        "    if self.aug_list:\n",
        "      do_aug = np.random.choice([True, False], 1, p=[self.aug_prob,\n",
        "                                                     1-self.aug_prob])\n",
        "      if do_aug:\n",
        "        aug_name = np.random.choice(self.aug_list, 1)\n",
        "        img = aug_name[0](img)\n",
        "    img = (img - torch.mean(img)) / torch.std(img)\n",
        "    return img, mask"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0pU9_SjM0pax"
      },
      "source": [
        "def get_data_loaders(categories, path, file_lists,\n",
        "                     augment, aug_prob, batch_size):\n",
        "  \"\"\"\n",
        "  Wrapper function to return dataloader(s)\n",
        "  Args:\n",
        "    categories (List[str]): names of processes for which dataloader needed\n",
        "    path (str): path where images stored\n",
        "    file_lists (List[List[str]]): list of file lists\n",
        "    augment (boolean): whether to apply augmentation\n",
        "    aug_prob (float): Probability of applying random aug\n",
        "    batch_size (int): batch size\n",
        "  Returns:\n",
        "    torch.utils.data.DataLoader object\n",
        "  \"\"\"\n",
        "  loaders = []\n",
        "  for i, category in enumerate(categories):\n",
        "    if category == 'train' and augment:\n",
        "      aug_list = [\n",
        "          transforms.RandomAffine(0, translate=(0.2, 0.2)),\n",
        "          transforms.RandomHorizontalFlip(p=1),\n",
        "          transforms.RandomRotation(degrees=(-10, 10), fill=(0,)),\n",
        "          transforms.GaussianBlur((17, 17), (11, 11))\n",
        "      ]\n",
        "    else:\n",
        "      aug_list = None\n",
        "    loader = DataLoader(\n",
        "        ImageLoader(path, file_lists[i], aug_list, aug_prob),\n",
        "        batch_size,\n",
        "        num_workers=1\n",
        "        )\n",
        "    loaders.append(loader)\n",
        "  return loaders"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xPhL86aF4sjb"
      },
      "source": [
        "## Train/val/test loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ihzXgpzJnz9b"
      },
      "source": [
        "# dice score\n",
        "def integral_dice(pred, gt, k):\n",
        "    '''\n",
        "    Dice coefficient for multiclass hard thresholded prediction consisting of integers instead of binary\n",
        "    values. k = integer for class for which Dice is being calculated.\n",
        "    '''\n",
        "    return (torch.sum(pred[gt == k] == k)*2.0\n",
        "            / (torch.sum(pred[pred == k] == k)\n",
        "               + torch.sum(gt[gt == k] == k)).float())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yLnI2nmj2XZK"
      },
      "source": [
        "def learn(model, loader, optimizer, process):\n",
        "  \"\"\" main function for single epoch of train, val or test \"\"\"\n",
        "  dice_list = []\n",
        "  running_loss = 0\n",
        "  num_batches = len(loader)\n",
        "  with trange(num_batches, desc=process, ncols=100) as t:\n",
        "    for batch_num, sample in enumerate(loader):\n",
        "      img_batch, masks = sample\n",
        "      masks = masks[:, 0, :, :, 0].long()\n",
        "      # one hot encoding labels\n",
        "      masks_oh = one_hot(masks, num_classes=2, device='cpu', dtype=masks.dtype)\n",
        "      if process == 'train':\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        preds = F.softmax(model(img_batch.cuda()), 1)\n",
        "        loss = F.binary_cross_entropy(preds, masks_oh.cuda())\n",
        "        # loss = dice_loss(preds, masks.cuda())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "      else:\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "          preds = F.softmax(model(img_batch.cuda()), 1)\n",
        "          loss = F.binary_cross_entropy(preds, masks_oh.cuda())\n",
        "          # loss = dice_loss(preds, masks.cuda())\n",
        "      hard_preds = torch.argmax(preds, 1)\n",
        "      dice = integral_dice(hard_preds, masks, 1)\n",
        "      dice_list.append(dice.item())\n",
        "      running_loss += loss  \n",
        "      t.set_postfix(loss=running_loss.item()/(float(batch_num+1)*batch_size))\n",
        "      t.update()\n",
        "  mean_dice = np.mean(np.array(dice_list))\n",
        "  final_loss = running_loss.item()/(num_batches*batch_size)\n",
        "  return mean_dice, final_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1QjMxW-3C41l"
      },
      "source": [
        "def get_splits(all_names, train_size, val_size, test_size):\n",
        "  split1_size = (val_size+test_size)\n",
        "  split2_size = test_size / (val_size+test_size)\n",
        "  trn_names, valtst_names = train_test_split(\n",
        "      all_names, test_size=split1_size, random_state=0)\n",
        "  val_names, tst_names = train_test_split(\n",
        "      valtst_names, test_size=split2_size, random_state=0)\n",
        "  return trn_names, val_names, tst_names "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6SsSa05k__2U"
      },
      "source": [
        "def perform_learning(model, optimizer, path, all_names, batch_size,\n",
        "                     splits, num_epochs):\n",
        "  \"\"\" Wrapper function to run train, val, test loops \"\"\"\n",
        "  train_size, val_size, test_size = splits\n",
        "  trn_names, val_names, tst_names = get_splits(all_names, train_size, val_size,\n",
        "                                               test_size)\n",
        "  train_loader, val_loader, test_loader = get_data_loaders(\n",
        "      ['train', 'val', 'test'],\n",
        "      path, [trn_names, val_names, tst_names],\n",
        "      augment=True,\n",
        "      aug_prob=0.5,\n",
        "      batch_size=batch_size\n",
        "      )\n",
        "  for epoch_num in range(num_epochs):\n",
        "    train_dice, train_loss = learn(model, train_loader, optimizer, 'train')\n",
        "    print(f'Training Epoch {epoch_num} - Loss: {train_loss} ; Dice : {train_dice}')\n",
        "    val_dice, val_loss = learn(model, val_loader, optimizer, 'val')\n",
        "    print(f'Validation Epoch {epoch_num} - Loss: {val_loss} ; Dice : {val_dice}')\n",
        "  tst_dice, tst_loss = learn(model, test_loader, optimizer, 'test')\n",
        "  print(f'Test - Loss: {tst_loss} ; Dice : {tst_dice}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n4llZPlvHbxa"
      },
      "source": [
        "## Let's run!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-WMbMigmHVhn",
        "outputId": "b1aec82a-dec2-4cf4-c764-3df056bd646c"
      },
      "source": [
        "path = '/content/us_dataset'\n",
        "all_names = os.listdir(os.path.join(path, 'all_images'))\n",
        "\n",
        "lr = 1e-4\n",
        "wt_dec = 1e-4\n",
        "num_epochs = 5\n",
        "batch_size = 2\n",
        "splits = [0.8, 0.1, 0.1]\n",
        "\n",
        "model = unet(n_classes=2)\n",
        "model = model.cuda()\n",
        "\n",
        "optimizer = Adam(model.parameters(), lr=lr, weight_decay=wt_dec)\n",
        "\n",
        "perform_learning(model, optimizer, path, all_names, batch_size,\n",
        "                 splits, num_epochs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\rtrain:   0%|                                                                | 0/400 [00:00<?, ?it/s]Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f77203b0050>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "  File \"/usr/lib/python3.7/multiprocessing/process.py\", line 151, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "AssertionError: can only test a child process\n",
            "train: 100%|██████████████████████████████████████████| 400/400 [04:32<00:00,  1.47it/s, loss=0.248]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training Epoch 0 - Loss: 0.2480936813354492 ; Dice : 0.3468497826378143\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "val: 100%|██████████████████████████████████████████████| 50/50 [00:30<00:00,  1.64it/s, loss=0.208]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation Epoch 0 - Loss: 0.20792417526245116 ; Dice : 0.663888944387436\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "train: 100%|██████████████████████████████████████████| 400/400 [04:42<00:00,  1.42it/s, loss=0.187]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training Epoch 1 - Loss: 0.1873435401916504 ; Dice : 0.7037050391174853\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "val: 100%|██████████████████████████████████████████████| 50/50 [00:30<00:00,  1.62it/s, loss=0.144]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation Epoch 1 - Loss: 0.14365605354309083 ; Dice : 0.7724949312210083\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "train: 100%|██████████████████████████████████████████| 400/400 [04:32<00:00,  1.47it/s, loss=0.165]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training Epoch 2 - Loss: 0.1648356246948242 ; Dice : 0.7402136340644211\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "val: 100%|██████████████████████████████████████████████| 50/50 [00:30<00:00,  1.64it/s, loss=0.134]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation Epoch 2 - Loss: 0.13352821350097657 ; Dice : 0.7840890741348266\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "train: 100%|██████████████████████████████████████████| 400/400 [04:31<00:00,  1.47it/s, loss=0.155]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training Epoch 3 - Loss: 0.1545381736755371 ; Dice : 0.7611129674501718\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "val: 100%|██████████████████████████████████████████████| 50/50 [00:30<00:00,  1.66it/s, loss=0.119]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation Epoch 3 - Loss: 0.11851264953613282 ; Dice : 0.8111391639709473\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "train: 100%|██████████████████████████████████████████| 400/400 [04:29<00:00,  1.48it/s, loss=0.147]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training Epoch 4 - Loss: 0.14718589782714844 ; Dice : 0.7784149835258722\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "val: 100%|██████████████████████████████████████████████| 50/50 [00:30<00:00,  1.65it/s, loss=0.126]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation Epoch 4 - Loss: 0.1259153175354004 ; Dice : 0.7942298424243927\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "test: 100%|██████████████████████████████████████████████| 50/50 [00:30<00:00,  1.66it/s, loss=0.13]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test - Loss: 0.1300245475769043 ; Dice : 0.7992158937454223\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "owro90xgJyXi"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hySi7RIRigTS"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}